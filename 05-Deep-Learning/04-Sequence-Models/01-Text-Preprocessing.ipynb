{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Text preprocessing is an essential step in natural language processing (NLP) tasks. It involves transforming raw text data into a format that is more suitable for analysis and machine learning algorithms. In this tutorial, we will cover various common techniques for text preprocessing. Let's dive in!\n",
    "\n",
    "## Lowercasing\n",
    "Converting all text to lowercase can help to normalize the data and reduce the vocabulary size. It ensures that words in different cases are treated as the same word. For example, \"apple\" and \"Apple\" will both be transformed to \"apple\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting text to lowercase.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Converting Text to Lowercase.\"\n",
    "lowercased_sentence = sentence.lower()\n",
    "print(lowercased_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of Punctuation and Special Characters\n",
    "Punctuation marks and special characters often do not add much meaning to the text and can be safely removed. Common punctuation marks include periods, commas, question marks, and exclamation marks. You can use regular expressions or string operations to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove all the punctuation marks Special characters\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sentence = \"Remove all the punctuation marks! @Special characters?\"\n",
    "cleaned_sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "print(cleaned_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Word Removal:\n",
    "Stop words are commonly occurring words in a language, such as \"a,\" \"an,\" \"the,\" \"is,\" and \"in.\" These words provide little semantic value and can be removed to reduce noise in the data. Libraries like NLTK provide a list of predefined stop words for different languages.\n",
    "\n",
    "Before using the code make sure you downloaded all the stopwords uning the first shell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove stop words sentence.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sentence = \"Remove the stop words from this sentence.\"\n",
    "filtered_words = [word for word in sentence.split() if word.lower() not in stop_words]\n",
    "filtered_sentence = ' '.join(filtered_words)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Contractions\n",
    "Contractions are shortened versions of words, such as \"can't\" (cannot) or \"it's\" (it is). Depending on your analysis requirements, you may choose to expand contractions to their full forms or leave them as they are. Expanding contractions can be done using predefined mapping dictionaries or rule-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot believe it is true.\n"
     ]
    }
   ],
   "source": [
    "contractions = {\n",
    "    \"can't\": \"cannot\",\n",
    "    \"it's\": \"it is\"\n",
    "}\n",
    "sentence = \"I can't believe it's true.\"\n",
    "words = sentence.split()\n",
    "expanded_words = [contractions.get(word, word) for word in words]\n",
    "expanded_sentence = ' '.join(expanded_words)\n",
    "print(expanded_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling HTML Tags\n",
    "If your text data contains HTML tags, you might want to remove them. You can use libraries like BeautifulSoup or regular expressions to extract the text content and discard the HTML tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove HTML tags from this text.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "html_content = \"<p>Remove <b>HTML tags</b> from this text.</p>\"\n",
    "cleaned_text = re.sub(r'<.*?>', '', html_content)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling URLs\n",
    "URLs often appear in text data and may not provide meaningful information for many NLP tasks. You can remove URLs using regular expressions or replace them with a placeholder like \"URL\" to indicate their presence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visit my website at  for more information.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sentence = \"Visit my website at https://www.example.com for more information.\"\n",
    "cleaned_sentence = re.sub(r'http\\S+|www.\\S+', '', sentence)\n",
    "print(cleaned_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Emojis and Special Characters\n",
    "Emojis and special characters can be common in text data, particularly in social media texts. Depending on your analysis needs, you can remove them or replace them with appropriate placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love pizza \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sentence = \"I love pizza! ðŸ˜ðŸ•\"\n",
    "cleaned_sentence = re.sub('[^a-zA-Z0-9\\s]', '', sentence)\n",
    "print(cleaned_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell Checking\n",
    "Spelling errors are common in unprocessed text. Using spell checking techniques or libraries like pyspellchecker, you can correct common spelling mistakes and improve the quality of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this sentence has spelling errors\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "sentence = \"Thhis sentennce hass spelllingg erroors.\"\n",
    "corrected_words = [spell.correction(word) for word in sentence.split()]\n",
    "corrected_sentence = ' '.join(corrected_words)\n",
    "print(corrected_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Rare Words\n",
    "In some cases, you may want to remove or replace rare words to reduce the vocabulary size. Rare words can be defined based on their frequency of occurrence in the corpus. You can replace them with a special token like \"UNK\" (unknown) or remove them altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK UNK UNK UNK sentence\n",
      "UNK sentence UNK UNK\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "sentences = [\"This is a sample sentence\", \"Another sentence for demonstration\"]\n",
    "\n",
    "# Tokenization\n",
    "tokens = [word.lower() for sentence in sentences for word in sentence.split()]\n",
    "\n",
    "# Calculate word frequencies\n",
    "word_freq = dict(Counter(tokens))\n",
    "\n",
    "# Define the threshold for rare words\n",
    "threshold = 2\n",
    "\n",
    "# Replace rare words with 'UNK' token\n",
    "# OOV tokens = Out-of-Vocabulary tokens\n",
    "filtered_tokens = [\n",
    "    [word if word_freq.get(word.lower(), 0) >= threshold else 'UNK' for word in sentence.split()]\n",
    "    for sentence in sentences\n",
    "]\n",
    "\n",
    "# Print the result\n",
    "for sentence in filtered_tokens:\n",
    "    print(' '.join(sentence))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is the process of breaking down a piece of text into smaller units called tokens. These tokens can be words, subwords, or even characters, depending on the level of granularity desired. Tokenization is a fundamental step in text preprocessing and is crucial for various natural language processing (NLP) tasks, such as machine translation, sentiment analysis, and language generation.\n",
    "\n",
    "Here's a detailed explanation of tokenization:\n",
    "\n",
    "### Word Tokenization\n",
    "Word tokenization is the most common form of tokenization, where the text is split into individual words. For example, given the sentence \"Tokenization is important for NLP tasks,\" the word tokens would be: [\"Tokenization\", \"is\", \"important\", \"for\", \"NLP\", \"tasks\"].\n",
    "\n",
    "Word tokenization is typically performed using whitespace as the delimiter. However, it's important to handle cases like punctuation marks, contractions, and hyphenated words correctly. For example, \"don't\" should be tokenized as [\"do\", \"n't\"] instead of [\"don\", \"'\", \"t\"].\n",
    "\n",
    "Libraries like NLTK, spaCy, and the tokenizers package provide ready-to-use word tokenization functions.\n",
    "\n",
    "***\n",
    "Before running any of these tokenization techniques, make sure you have `punkt` downloaded. `punkt` refers to the Punkt Tokenizer, which is a pre-trained unsupervised machine learning model for sentence tokenization. The NLTK Punkt Tokenizer is trained on large corpora and is capable of handling a wide range of sentence boundary detection for multiple languages. It uses a combination of rule-based heuristics and statistical models to identify sentence boundaries accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\FM-PC-\n",
      "[nltk_data]     LT-279\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'important', 'for', 'NLP', 'tasks']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = \"Tokenization is important for NLP tasks\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword Tokenization\n",
    "Subword tokenization breaks the text into smaller units that may not necessarily correspond to complete words. This technique is particularly useful for languages with complex morphology or for handling out-of-vocabulary words.\n",
    "\n",
    "One popular subword tokenization algorithm is Byte Pair Encoding (BPE), which iteratively merges the most frequent character pairs to create subword tokens. For example, the word \"unhappiness\" might be tokenized as [\"un\", \"happiness\"].\n",
    "\n",
    "Another widely used subword tokenization approach is WordPiece, which is similar to BPE but ensures that the tokenizer treats whole words as single tokens. This is particularly helpful for languages like Chinese, where characters don't have clear word boundaries.\n",
    "\n",
    "The Hugging Face tokenizers library provides efficient implementations of BPE and WordPiece tokenization.\n",
    "\n",
    "Before running the program below, make sure you have `tokenizers` installed on your device.\n",
    "\n",
    "`pip install tokenizers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'ken', 'ization']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=[\"./../../data/test-data.txt\"], vocab_size=1000)\n",
    "\n",
    "text = \"tokenization\"\n",
    "encoding = tokenizer.encode(text)\n",
    "tokens = encoding.tokens\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Tokenization\n",
    "In some cases, it may be necessary to tokenize text at the character level. This approach treats each individual character as a token. It is useful for tasks such as text generation or processing languages without clear word boundaries.\n",
    "\n",
    "For example, the sentence \"Hello, world!\" would be tokenized as [\"H\", \"e\", \"l\", \"l\", \"o\", \",\", \" \", \"w\", \"o\", \"r\", \"l\", \"d\", \"!\"].\n",
    "\n",
    "Character tokenization can be performed easily using string manipulation functions available in most programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world!\"\n",
    "tokens = list(text)\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "Stemming and lemmatization are techniques used in natural language processing (NLP) to reduce words to their base or root forms. Both approaches aim to normalize words and reduce inflectional variations, enabling better analysis and comparison of words. However, they differ in their methods and outputs. Let's dive into each technique in detail:\n",
    "\n",
    "### Stemming\n",
    "Stemming is a process of reducing words to their base or root forms by removing prefixes or suffixes. The resulting form is often a stem, which may not be an actual word itself. The primary goal of stemming is to simplify the vocabulary and group together words with the same base meaning.\n",
    "\n",
    "For example, when using a stemming algorithm on the words \"running,\" \"runs,\" and \"ran,\" the common stem would be \"run.\" The stemming process cuts off the suffixes (\"-ning,\" \"-s,\" and \"-\"), leaving behind the core form of the word.\n",
    "\n",
    "Stemming algorithms follow simple rules and heuristics based on linguistic patterns, rather than considering the context or part of speech of the word. Some popular stemming algorithms include the Porter stemming algorithm, the Snowball stemmer (which supports multiple languages), and the Lancaster stemming algorithm.\n",
    "\n",
    "Stemming is a computationally lightweight approach and can be useful in certain cases where the exact word form is not crucial. However, it may produce stems that are not actual words, leading to potential loss of meaning and ambiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tasti\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "word = \"tasty\"\n",
    "stemmed_word = stemmer.stem(word)\n",
    "print(stemmed_word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Lemmatization, on the other hand, aims to reduce words to their canonical or dictionary forms, known as lemmas. Unlike stemming, lemmatization considers the context and part of speech (POS) of the word to generate meaningful lemmas. The resulting lemmas are actual words found in the language's dictionary.\n",
    "\n",
    "For example, when lemmatizing the words \"running,\" \"runs,\" and \"ran,\" the lemma for each would be \"run.\" Lemmatization takes into account the POS information to accurately determine the base form of the word.\n",
    "\n",
    "Lemmatization algorithms use linguistic rules and morphological analysis to identify the appropriate lemma. They often rely on language-specific resources, such as word lists and morphological databases. Some popular lemmatization tools include the WordNet lemmatizer and the spaCy library (which supports lemmatization for multiple languages).\n",
    "\n",
    "Lemmatization typically produces more accurate and meaningful results compared to stemming because it retains the core meaning of words. It is especially useful in tasks that require precise word analysis, such as information retrieval, question answering, and sentiment analysis.\n",
    "\n",
    "However, lemmatization can be more computationally intensive compared to stemming due to its reliance on POS tagging and language-specific resources.\n",
    "\n",
    "Before running any of these tokenization techniques, make sure you have `wordnet` downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\FM-PC-\n",
      "[nltk_data]     LT-279\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tasty\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word = \"tasty\"\n",
    "lemmatized_word = lemmatizer.lemmatize(word)\n",
    "print(lemmatized_word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When deciding between stemming and lemmatization, consider the trade-off between simplicity and accuracy. If you require speed and a broad reduction of word forms, stemming may be sufficient. However, if you need more accurate analysis and want to preserve the semantic meaning of words, lemmatization is generally the preferred choice.\n",
    "\n",
    "It's important to note that both stemming and lemmatization have limitations. They may not always produce the correct base forms, especially for irregular words or those not present in the chosen language's dictionary. Contextual information, such as word sense disambiguation, can further enhance the accuracy of both techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
