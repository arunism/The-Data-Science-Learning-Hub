{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Transfer learning is a machine learning technique that involves leveraging the knowledge gained from training a model on one task and applying it to another related task. Instead of starting the training process from scratch, transfer learning enables the model to benefit from the previously learned features or representations.\n",
    "\n",
    "The basic idea behind transfer learning is that the features learned by a model on a large and diverse dataset can be generalized and useful for other similar tasks. By transferring this knowledge, models can achieve better performance and require less training time and data compared to training from scratch.\n",
    "\n",
    "Transfer learning involves two main steps:\n",
    "\n",
    "- **Pre-training:** A model is trained on a source task using a large dataset. This task is typically unrelated or indirectly related to the target task. The pre-training step enables the model to learn generic features or representations that capture useful patterns in the data.\n",
    "\n",
    "- **Fine-tuning:** The pre-trained model is then used as a starting point for training on the target task. The model is further trained on a smaller, task-specific dataset related to the target task. During fine-tuning, the model's parameters are updated to adapt to the specifics of the new task while preserving the previously learned knowledge.\n",
    "\n",
    "There are different ways to perform transfer learning, depending on the availability of data and similarity between the source and target tasks:\n",
    "\n",
    "- **Feature Extraction:** In this approach, the pre-trained model's layers are frozen, and only the output layers are replaced or added for the target task. The pre-trained model serves as a feature extractor, and the new layers are trained to map the extracted features to the target task's specific classes or labels.\n",
    "\n",
    "- **Fine-tuning:** In this approach, both the pre-trained model's layers and the new layers are trained together on the target task. The initial layers of the pre-trained model can be frozen, while the deeper layers closer to the task-specific layers are fine-tuned. This allows the model to adapt the previously learned features to the target task.\n",
    "\n",
    "Transfer learning is especially beneficial in scenarios where:\n",
    "\n",
    "- The target task has a limited amount of labeled data, as transfer learning can effectively leverage knowledge from a larger source dataset.\n",
    "- The source and target tasks share some common underlying patterns or features, allowing the model to transfer relevant knowledge.\n",
    "- Training a model from scratch on the target task would be computationally expensive or time-consuming.\n",
    "\n",
    "By utilizing transfer learning, models can achieve improved performance, faster convergence, and reduced data requirements, making it a valuable technique in various machine learning applications.\n",
    "\n",
    "**How is Transfer Lerning done in CNN Models?**\n",
    "\n",
    "Transfer learning in CNNs (Convolutional Neural Networks) typically involves using a pre-trained CNN model as a starting point and adapting it to a new task. Here are the steps involved in transfer learning using CNNs:\n",
    "\n",
    "- **Pre-trained Model Selection:** Choose a pre-trained CNN model that has been trained on a large and diverse dataset, such as ImageNet. Popular choices include YOLO, VGG, ResNet, Inception, or MobileNet. These models have learned general image features that can be applicable to a wide range of tasks.\n",
    "- **Removing the Classifier:** Remove the fully connected layers (classifier) at the end of the pre-trained model. These layers are task-specific and need to be replaced with new layers suitable for the new task.\n",
    "- **Feature Extraction:** Freeze the weights of the pre-trained layers to prevent them from being updated during training. These layers already contain valuable learned features, and freezing them allows the model to act as a feature extractor.\n",
    "- **Adding New Layers:** Add new layers on top of the pre-trained layers. These new layers will be specific to the new task. The number of new layers and their architecture can vary based on the complexity of the task and the available data.\n",
    "- **Training:** Train the model using the new task-specific dataset. The training is performed on the new layers while keeping the pre-trained layers frozen. Only the weights of the new layers are updated during training.\n",
    "- **Fine-tuning (Optional):** If the new task dataset is large enough and similar to the original dataset used for pre-training, fine-tuning can be performed. In this step, the weights of the pre-trained layers closest to the new layers are unfrozen, allowing them to be further fine-tuned along with the new layers. This fine-tuning enables the model to adapt more specifically to the new task.\n",
    "- **Evaluation and Testing:** Evaluate the performance of the trained model on a validation set. Adjust the hyperparameters and model architecture if necessary. Once satisfied with the model's performance, test it on unseen data to assess its generalization ability."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning using YOLO\n",
    "\n",
    "YOLO is a popular object detection algorithm that can detect and classify objects in real-time. It has gained widespread recognition for its speed and accuracy, making it suitable for various applications, including autonomous vehicles, surveillance systems, and robotics. YOLO takes an input image and divides it into a grid, predicting bounding boxes and class probabilities for each grid cell.\n",
    "\n",
    "### History of YOLO\n",
    "YOLO was introduced in 2016 by Joseph Redmon, et al. Since its inception, several versions of YOLO have been developed, including YOLOv2, YOLOv3, YOLOv4, YOLOv5, YOLOv6, YOLOv7, YOLOv8 and it is still evolving. Each version aimed to improve object detection accuracy and speed.\n",
    "\n",
    "### YOLO Model Architecture\n",
    "\n",
    "Here we will talk about YOLO V4 model architecture. The YOLO V4 architecture consists of two main components: the backbone network and the detection network.\n",
    "\n",
    "![YOLO](./../../assets/yolo.jpg)\n",
    "\n",
    "- **Backbone Network:** The backbone network is responsible for extracting features from the input image. It typically uses a convolutional neural network (CNN) such as DarkNet, which consists of several convolutional and pooling layers. The backbone network progressively reduces the spatial dimensions of the input while capturing meaningful features.\n",
    "\n",
    "- **Detection Network:** The detection network takes the features extracted by the backbone network and performs object detection. It predicts bounding boxes and class probabilities for objects within each grid cell.\n",
    "\n",
    "The detection network consists of the following components:\n",
    "\n",
    "- 1x1 Convolutional Layers: These layers reduce the depth dimension of the input features.\n",
    "- 3x3 Convolutional Layers: These layers detect objects at different scales and aspect ratios.\n",
    "- Prediction Layers: Each prediction layer predicts bounding box coordinates and class probabilities. YOLO predicts bounding boxes as offsets from the grid cell boundaries.\n",
    "\n",
    "For more visit this [link](https://iq.opengenus.org/yolov4-model-architecture/).\n",
    "\n",
    "###  Input Format\n",
    "The YOLO model requires input images of fixed size. Common input sizes include 416x416, 608x608, or 1024x1024 pixels. Larger input sizes generally lead to better detection accuracy but slower inference times.\n",
    "\n",
    "### Output Format\n",
    "The YOLO model outputs bounding box predictions and class probabilities. Each bounding box prediction consists of four coordinates (x, y, width, height) and an associated confidence score. Class probabilities represent the likelihood of each detected object belonging to a specific class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Transfer Learning\n",
    "\n",
    "TensorFlow provides a collection of pre-trained models through the `tf.keras.applications` module. For this example, we'll use the VGG16 model.\n",
    "\n",
    "Here's a step-by-step guide to performing transfer learning using the CIFAR dataset:\n",
    "\n",
    "Step 1:  Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Load and preprocess the CIFAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to the range of 0 and 1\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Load the pre-trained VGG16 model (excluding the fully connected layers) and freeze its weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "\n",
    "# Freeze the pre-trained weights\n",
    "base_model.trainable = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Build your custom model on top of the pre-trained base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 108s 137ms/step - loss: 1.3655 - accuracy: 0.5266 - val_loss: 1.2787 - val_accuracy: 0.5516\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 105s 135ms/step - loss: 1.1834 - accuracy: 0.5846 - val_loss: 1.2042 - val_accuracy: 0.5772\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 108s 138ms/step - loss: 1.1209 - accuracy: 0.6083 - val_loss: 1.1718 - val_accuracy: 0.5899\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 116s 149ms/step - loss: 1.0791 - accuracy: 0.6229 - val_loss: 1.1404 - val_accuracy: 0.5969\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 113s 145ms/step - loss: 1.0393 - accuracy: 0.6380 - val_loss: 1.1272 - val_accuracy: 0.6019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21034613f90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=5, validation_data=(x_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 18s 57ms/step - loss: 1.1272 - accuracy: 0.6019\n",
      "Test Loss: 1.1271916627883911\n",
      "Test Accuracy: 0.6018999814987183\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Perform inference on new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 265ms/step\n",
      "Predicted Class: 9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load a new image for inference\n",
    "new_image_path = './../../assets/test-image.jpg'\n",
    "new_image = image.load_img(new_image_path, target_size=(32, 32))\n",
    "new_image = image.img_to_array(new_image)\n",
    "new_image = np.expand_dims(new_image, axis=0)\n",
    "new_image = new_image / 255.0\n",
    "\n",
    "# Perform inference\n",
    "predictions = model.predict(new_image)\n",
    "predicted_class = np.argmax(predictions)\n",
    "\n",
    "print(\"Predicted Class:\", predicted_class)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process allows the model to leverage the pre-trained weights from VGG16 while training the added fully connected layers specifically for the CIFAR dataset. Feel free to modify the architecture or hyperparameters according to your needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
