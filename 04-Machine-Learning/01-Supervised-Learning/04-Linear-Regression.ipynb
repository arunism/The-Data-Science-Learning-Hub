{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear regression is a widely used statistical technique for modeling the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship between the variables and aims to find the best-fit line that minimizes the difference between the predicted and actual values. The goal of linear regression is to make predictions or understand the impact of independent variables on the dependent variable.\n",
    "\n",
    "## Simple Linear Regression\n",
    "\n",
    "In simple linear regression, we consider a single independent variable and a single dependent variable. The relationship between the variables can be represented by the equation:\n",
    "\n",
    "$y = mx + c$\n",
    "\n",
    "Where:\n",
    "\n",
    "- `y` is the dependent variable\n",
    "- `x` is the independent variable\n",
    "- `c` is the y-intercept (the value of `y` when `x` is 0)\n",
    "- `m` is the slope (the change in `y` for a unit change in `x`)\n",
    "\n",
    "In higher dimension this equation becomes:\n",
    "\n",
    "$y = wx + b$\n",
    "\n",
    "The goal is to estimate the values of $w$ and $b$ that best fit the data.\n",
    "\n",
    "## Geometric and Mathematical Intuition\n",
    "\n",
    "Geometrically, linear regression aims to find the line that minimizes the sum of squared distances between the observed data points and the predicted values on the line. The line is chosen such that the vertical distances between the points and the line are minimized.\n",
    "\n",
    "![Linear Regression](./../../assets/linear-regression.jpg)\n",
    "\n",
    "$error for x_1 = y_1 - ŷ_1$\n",
    "\n",
    "$error for x_2 = y_2 - ŷ_2$\n",
    "\n",
    "$error for x_3 = y_3 - ŷ_3 = 0 \\quad$ *$∵ [y_3 - ŷ_3]$*\n",
    "\n",
    "Mathematically, the goal is to minimize the sum of squared residuals (also known as the residual sum of squares or RSS) given by:\n",
    "\n",
    "$RSS = Σ(y - ŷ)^2$\n",
    "\n",
    "Where:\n",
    "\n",
    "- `y` is the observed value of the dependent variable\n",
    "- `ŷ` is the predicted value of the dependent variable based on the regression line\n",
    "\n",
    "The best-fit line is obtained by minimizing RSS, which can be achieved through various optimization techniques.\n",
    "\n",
    "## Ordinary Least Squares (OLS) Estimation\n",
    "\n",
    "The most common method to estimate the coefficients (`b` and `w`) in linear regression is the Ordinary Least Squares (OLS) estimation. It aims to minimize the sum of squared residuals by finding the values of `b` and `w` that minimize the following equation:\n",
    "\n",
    "$\\frac{∂RSS}{∂b0} = -2Σ(y - b0 - b1 * x) = 0$\n",
    "\n",
    "$\\frac{∂RSS}{∂b1} = -2Σx(y - b0 - b1 * x) = 0$\n",
    "\n",
    "Solving these equations simultaneously will yield the estimated coefficients b0 and b1:\n",
    "\n",
    "$w = \\frac{Σ(x - x̄)(y - ȳ)}{Σ(x - x̄)^2}$\n",
    "\n",
    "$b = ȳ - w * x̄$\n",
    "\n",
    "Where:\n",
    "\n",
    "- `x̄` is the mean of the independent variable `x`\n",
    "- `ȳ` is the mean of the dependent variable `y`\n",
    "\n",
    "These formulas can be computed efficiently, providing the best-fit line."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "In multiple linear regression, we extend the concept of simple linear regression to include multiple independent variables. The equation for multiple linear regression can be expressed as:\n",
    "\n",
    "$y = b + w_1 * x_1 + w_2 * x_2 + ... + w_n * x_n$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $y$ is the dependent variable\n",
    "- $x_1, x_2, ..., x_n$ are the independent variables\n",
    "- $b$ is the y-intercept\n",
    "- $w_1, w_2, ..., w_n$ are the coefficients for each independent variable\n",
    "\n",
    "The goal is to estimate the values of the coefficients that provide the best-fit hyperplane.\n",
    "\n",
    "### Matrix Formulation\n",
    "\n",
    "To solve multiple linear regression efficiently, we can express it in matrix form. Let's define:\n",
    "\n",
    "- `Y` as a column vector of the dependent variable\n",
    "- `X` as a matrix of independent variables\n",
    "- `B` as a column vector of coefficients\n",
    "- `E` as a column vector of residuals\n",
    "\n",
    "The equation can be rewritten as:\n",
    "\n",
    "$Y = X * B + E$\n",
    "\n",
    "We aim to find the values of B that minimize the sum of squared residuals."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Regularization is a technique used to prevent overfitting and improve the generalization of the linear regression model. Two commonly used regularization methods are Ridge regression and Lasso regression.\n",
    "\n",
    "### Ridge Regression\n",
    "See *Logistic Regression* notebook for more\n",
    "\n",
    "### Lasso Regression\n",
    "See *Logistic Regression* notebook for more"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Methods\n",
    "\n",
    "To estimate the coefficients efficiently, optimization methods are used. Two common optimization algorithms are Gradient Descent and Normal Equations.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Gradient Descent is an iterative optimization algorithm that updates the coefficients gradually by descending along the negative gradient of the cost function. The cost function is typically the RSS, and the algorithm seeks to minimize it. The steps of Gradient Descent include:\n",
    "\n",
    "1. Initialize the coefficients randomly or with some initial guess.\n",
    "2. Calculate the predicted values using the current coefficients.\n",
    "3. Calculate the gradient of the cost function with respect to each coefficient.\n",
    "4. Update the coefficients by taking a step proportional to the negative gradient.\n",
    "5. Repeat steps 2-4 until convergence is reached.\n",
    "\n",
    "## Model Evaluation\n",
    "After fitting the linear regression model, it's important to evaluate its performance and assess its quality. Here are some commonly used evaluation metrics:\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "MSE measures the average squared difference between the predicted and actual values. It is computed as:\n",
    "\n",
    "$MSE = \\frac{Σ(y - ŷ)^2}{n}$\n",
    "\n",
    "Where `n` is the number of data points.\n",
    "\n",
    "### R-squared ($R^2$)\n",
    "R-squared represents the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, where 1 indicates a perfect fit. It is calculated as:\n",
    "\n",
    "$R^2 = 1 - \\frac{RSS}{TSS}$\n",
    "\n",
    "Where RSS is the residual sum of squares and TSS is the total sum of squares.\n",
    "\n",
    "The RSS (Residual Sum of Squares) represents the sum of squared differences between the observed dependent variable values (y) and the predicted values (ŷ) obtained from the linear regression model. Mathematically, it is calculated as follows:\n",
    "\n",
    "$RSS = Σ(y - ŷ)^2$\n",
    "\n",
    "On the other hand, the TSS (Total Sum of Squares) represents the total variation in the dependent variable (y) from its mean (ȳ). It measures the sum of squared differences between each observed dependent variable value (y) and the mean of the dependent variable (ȳ). Mathematically, it is calculated as follows:\n",
    "\n",
    "$TSS = Σ(y - ȳ)^2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [What Is Linear Regression? Types, Equation, Examples, and Best Practices](https://www.spiceworks.com/tech/artificial-intelligence/articles/what-is-linear-regression/)\n",
    "- [What is linear regression?](https://www.ibm.com/topics/linear-regression)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
