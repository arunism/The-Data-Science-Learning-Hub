{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm\n",
    "\n",
    "Naive Bayes is a simple yet powerful classification algorithm based on Bayes' theorem. It is widely used in various machine learning applications, such as text classification, spam filtering, sentiment analysis, and recommendation systems.\n",
    "\n",
    "**Assumption:** Each feature is conditionally independent of every other feature.\n",
    "\n",
    "## Inspiration behind Naive Bayes\n",
    "\n",
    "Naive Bayes is inspired by Bayes' theorem, which is a fundamental concept in probability theory. Bayes' theorem allows us to update our beliefs about an event based on new evidence. It calculates the probability of an event given prior knowledge or evidence.\n",
    "\n",
    "Bayes' theorem is defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "P(A|B) = \\frac{P(B|A) * P(A)}{P(B)}\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    "\n",
    "- $P(A|B)$ is the probability of event $A$ occurring given that event $B$ has occurred.\n",
    "- $P(B|A)$ is the probability of event $B$ occurring given that event $A$ has occurred.\n",
    "- $P(A)$ and $P(B)$ are the probabilities of events $A$ and $B$ occurring independently.\n",
    "\n",
    "To derive the Naive Bayes equation using Bayes' theorem, let's consider a classification problem with a set of features or predictors $X = {X1, X2, ..., Xn}$ and a target class $C$. We want to calculate the probability of the class $C$ given the features $X$, denoted as $P(C|X)$.\n",
    "\n",
    "\\begin{align*}\n",
    "P(C|X) &= \\frac{P(X|C) \\cdot P(C)}{P(X)} \\\\\n",
    "&= \\frac{P(X_1, X_2, \\ldots, X_n|C) \\cdot P(C)}{P(X)} \\\\\n",
    "&= \\frac{P(X_1|C) \\cdot P(X_2|C) \\cdot \\ldots \\cdot P(X_n|C) \\cdot P(C)}{P(X)} \\\\\n",
    "&= \\frac{P(X_1|C) \\cdot P(X_2|C) \\cdot \\ldots \\cdot P(X_n|C) \\cdot P(C)}{P(X)} \\\\\n",
    "&= \\frac{P(X_1|C) \\cdot P(X_2|C) \\cdot \\ldots \\cdot P(X_n|C) \\cdot P(C)}{P(X)} \\\\\n",
    "&= \\frac{P(C) \\cdot \\prod P(X_i|C)}{P(X)}\n",
    "\\end{align*}\n",
    "\n",
    "The final class label of $X$ is the class that maximizes the value of $P(C|X)$.\n",
    "\n",
    "## Naive Bayes Best and Worst Case Senarios\n",
    "\n",
    "**Best Case:** Naive Bayes performs well when the feature independence assumption holds true or is close to being true. It works particularly well with text classification tasks, where the features are often words or word frequencies. Naive Bayes can handle high-dimensional data efficiently and can provide good results even with limited training data.\n",
    "\n",
    "**Worst Case:** The major limitation of Naive Bayes is its assumption of feature independence. If the features are highly correlated, this assumption may not hold, leading to suboptimal results. In such cases, more sophisticated algorithms like logistic regression or decision trees may be more appropriate. Additionally, if there is insufficient training data or if the data violates the independence assumption to a large extent, Naive Bayes may perform poorly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace Additive Smoothing\n",
    "\n",
    "In case of Naive Bayes, we mostly deal with textual data. So in some scenarios, we may arise with the condition where we encouter a word $w_o$ that is not present in our training data.\n",
    "\n",
    "$P(C|X) = \\frac{P(C) \\cdot P(w_1|C) \\cdot P(w_2|C) \\cdot \\ldots \\cdot P(w_o|C)}{P(w)}$\n",
    "\n",
    "- If we drop this word $w_o$ then from above equation we get $P(w_o|C) = 1$, which cannot be true.\n",
    "- If we calculate its probability as usual, $P(w_o|C) = 0$, which makes the entire equation $0$.\n",
    "\n",
    "To overcome this issue, Laplace Additive Smooting comes into play, which suggest the following equation:\n",
    "\n",
    "$P(w_o|C) = \\frac{n + \\alpha}{N + \\alpha \\cdot k}$\n",
    "\n",
    "where,\n",
    "\n",
    "- n = Number of training data containing word $w_o$\n",
    "- $\\alpha$ = Hyperparameter (typically $\\alpha$ = 1)\n",
    "- N = Total number of training data\n",
    "- k = Number of distinct values $w_o$ can take\n",
    "\n",
    "## Log Probabilities for Numerical Stability\n",
    "\n",
    "Probability lies between 0 and 1. When we multiply large number of probabilities numerical unstability occurs, i.e.\n",
    "\n",
    "$P = P1*P2*P3*P4 = 0.2*0.1*0.1*0.2 = 0.0004$\n",
    "\n",
    "In python double precision has only 16 significant values, which may overflow for a large sequence of words. So there is the possibility of numerical underflow and python starts rounding numbers which creates deviation in the result.\n",
    "\n",
    "We can use `log` to convert these numerical product to sum:\n",
    "\n",
    "$log(P) = log(P1*P2*P3*P4) = log(p1) + log(p2) + log(p3) + log(p4) = 0.2 + 0.1 +0.1 + 0.2 = 0.6$\n",
    "\n",
    "## Bias-Variance Tradeoff\n",
    "\n",
    "We know,\n",
    "\n",
    "- High bias -> Underfit\n",
    "- High variance -> Overfit\n",
    "\n",
    "In any algorithms hyperparameter play their role for bias-variance tradeoff. In case of Naive Bayes, $\\alpha$ (as discussed under Laplace Additive Smoothing) is the hyperparameter.\n",
    "\n",
    "Consider a balanced dataset with 1000 positive (y=1) and 1000 negative (y=0) data points. Then the probability of any word $w_i$, (let's say $w_i$ occured twice in the positive training data) to belong to positive class is given by:\n",
    "\n",
    "$P(w_i|y=1) = \\frac{n + \\alpha}{N + \\alpha \\cdot k} = \\frac{n + \\alpha}{N + \\alpha \\cdot k}$\n",
    "\n",
    "**Case I:** When $\\alpha$ is small (let's say $\\alpha$=0)\n",
    "\n",
    "$P(w_i|y=1) = \\frac{2 + 0}{1000 + 0 \\cdot 2} = \\frac{2}{1000} = 0.002$\n",
    "\n",
    "Here we took $k=2$ assuming $k$ can take two values `present` or `not present`.\n",
    "\n",
    "So when we remove $w_i$ from the training data, i.e. $n=0$, we are making a small change by removing 2 from the numerator. However this small change makes the overall  probability zero, which is a drastic change in probability. In another words a small change in training data created a huge change in output, i.e. variance is high. So, from this we can draw an insight that when $\\alpha$ is small, we are possibly overfitting.\n",
    "\n",
    "**Case II:** When $\\alpha$ is large (let's say $\\alpha$=100)\n",
    "\n",
    "$P(w_i|y=1) = \\frac{2 + 1000}{1000 + 1000 \\cdot 2} = \\frac{1002}{3000} = 0.334$\n",
    "\n",
    "When we remove $w_i$ from the training data, i.e. $n=0$, the probability becomes:\n",
    "\n",
    "$P(w_i|y=1) = \\frac{0 + 1000}{1000 + 1000 \\cdot 2} = \\frac{1000}{3000} = 0.333$\n",
    "\n",
    "This looks like removing $w_i$ from the training data creates almost no change in the output. Also the result of Naive Bayes classifier for both classes looks like:\n",
    "\n",
    "$P(y=1|w_1, w_2, \\ldots, w_n) = \\frac{P(y=1) \\cdot \\prod P(w_i|y=1)}{P(w)} ≈ \\frac{1}{2} \\cdot \\frac{P(y=1)}{P(w)}$\n",
    "\n",
    "$P(y=0|w_1, w_2, \\ldots, w_n) = \\frac{P(y=0) \\cdot \\prod P(w_i|y=0)}{P(w)} ≈ \\frac{1}{2} \\cdot \\frac{P(y=1)}{P(w)}$\n",
    "\n",
    "We see, in both the cases $\\prod P(w_i|y=...)$ becomes nearly equals to $\\frac{1}{2}$, no matter what the value of $w_i$ be. So our model does not have much to do i.e. we are underfitting."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
